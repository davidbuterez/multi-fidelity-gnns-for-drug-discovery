{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SD_PYTHON_SCRIPT_PATH = '.../train_sd.py'\n",
    "SD_GET_EMBEDDINGS_SCRIPT_PATH = '.../generate_sd_embs_preds.py'\n",
    "SD_DATASET_FILE_PATH = '.../AID{ds}/SD.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "atomic_num_dict = {'1259350-1224905': 53,\n",
    " '1259418-1259416': 80,\n",
    " '449756-435005': 80,\n",
    " '449762': 80,\n",
    " '1465': 53,\n",
    " '1259375-1259374': 80,\n",
    " '1949': 53,\n",
    " '1431-873': 53,\n",
    " '504329': 80,\n",
    " '1445': 53,\n",
    " '624273-588549': 80,\n",
    " '624326-602261': 80,\n",
    " '624330': 80,\n",
    " '504941-488895': 80,\n",
    " '720512-652162': 53,\n",
    " '624474-624304': 80,\n",
    " '493155-485273': 80,\n",
    " '435010-2221': 53,\n",
    " '463203-2650': 80,\n",
    " '1259420-1259416': 80,\n",
    " '2382-2098': 53,\n",
    " '687027-652154': 53,\n",
    " '504313-2732': 80}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_template = 'python {training_script_path} --data-path {data_path} --sd-label {sd_label} --node-latent-dim 50 --graph-latent-dim 64 --gnn-intermediate-dim {gnn_interim_dim} --out-dir {out_dir} --smiles-column neut-smiles  --max-atomic-number {max_atomic_number} --readout {readout} --id-column {id_column} --monitor-loss-name train_total_loss {vgae} --num-layers {num_layers} --conv {conv} --no-use-batch-norm --num-epochs 200 --name {ds}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SD training scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = True\n",
    "\n",
    "# Must be columns in the .csv files\n",
    "ID_COLUMN = 'CID'\n",
    "SD_LABEL = 'SD'\n",
    "\n",
    "MAIN_OUT_DIR = '.../out_SD/'\n",
    "SD_TRAINING_SCRIPTS_PATH = '.../SD_training_scripts/'\n",
    "\n",
    "for ds in atomic_num_dict.keys():\n",
    "    ### Type of graph layer/convolution.\n",
    "    for conv in ['GCN', 'GIN', 'PNA']:\n",
    "        ### Number of graph layers (depth of GNN).\n",
    "        for num_layers in [2, 3]:\n",
    "            ### Use VGAE (as in the paper) or a GNN alternative with no variational component. VGAE almost always works better for SD.\n",
    "            for vgae_type in ['VGAE', 'GNN']:\n",
    "                ### This is the Dense/MLP neural readout. The standard alternatives are 'global_add_pool', 'global_mean_pool', and 'global_max_pool'. The Set Transformer and GRU did not perform well on SD so they are not included at the moment.\n",
    "                for readout in ['linear']:\n",
    "                    ### Use edge features or not (edges features have a dimension of 13).\n",
    "                    for edges in ['--edge-dim 13', None]:\n",
    "\n",
    "                        ### GCN does not work with edges\n",
    "                        if conv == 'GCN' and edges is not None:\n",
    "                            continue\n",
    "                        \n",
    "                        vgae = '--use-vgae' if vgae_type == 'VGAE' else '--no-use-vgae'\n",
    "                        edges_path = 'EDGES' if edges is not None else 'NO-EDGES'\n",
    "\n",
    "                        gnn_interim_dim = 512 if conv != 'PNA' else 515\n",
    "\n",
    "                        ### Define a path for the checkpoints. This will be created if it does not exist.\n",
    "                        out_dir = os.path.join(MAIN_OUT_DIR, f'{ds}/{num_layers}/{vgae_type}/{conv}/{readout}/{edges_path}')\n",
    "                        Path(out_dir).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "                        ### Set input data path. This should be a single .csv file.\n",
    "                        data_path = SD_DATASET_FILE_PATH.format(ds=ds)\n",
    "\n",
    "                        ### Generate script with all the settings \n",
    "                        script = script_template.format(ds=ds, data_path=data_path, vgae=vgae, conv=conv, num_layers=num_layers, readout=readout, out_dir=out_dir, \n",
    "                            max_atomic_number=atomic_num_dict[ds], gnn_interim_dim=gnn_interim_dim, sd_label=SD_LABEL, id_column=ID_COLUMN,\n",
    "                            training_script_path=TRAIN_SD_PYTHON_SCRIPT_PATH)\n",
    "\n",
    "                        ### Add edges if value is set\n",
    "                        if edges_path == 'EDGES':\n",
    "                            script += ' --edge-dim 13'\n",
    "\n",
    "                        ### GPU or CPU\n",
    "                        if USE_CUDA:\n",
    "                            script += ' --use-cuda'\n",
    "                        else:\n",
    "                            script += ' --no-use-cuda'\n",
    "                        \n",
    "                        ### If we have checkpoints that we want to resume from. Disabled at the moment.\n",
    "                        # if (num_layers, conv, vgae_type, readout) in ckpt_dict:\n",
    "                        #     script += f' --ckpt-path {ckpt_dict[(num_layers, conv, vgae_type, readout)]}'\n",
    "\n",
    "                        ### Save training scripts in a dedicated directory.\n",
    "                        with open(os.path.join(SD_TRAINING_SCRIPTS_PATH, f'{ds}_{num_layers}_{conv}_{vgae_type}_{readout}_{edges_path}.sh'), 'w') as f:\n",
    "                            f.write(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SD generate embeddings and predictions scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_template = 'python {training_script_path} --data-path {data_path} --sd-label {sd_label} --node-latent-dim 50 --graph-latent-dim 64 --gnn-intermediate-dim {gnn_interim_dim} --out-dir {out_dir} --smiles-column neut-smiles  --max-atomic-number {max_atomic_number} --readout {readout} --id-column {id_column} --monitor-loss-name train_total_loss {vgae} --num-layers {num_layers} --conv {conv} --no-use-batch-norm --name {ds}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = True\n",
    "\n",
    "# Must be columns in the .csv files\n",
    "ID_COLUMN = 'CID'\n",
    "SD_LABEL = 'SD'\n",
    "\n",
    "MAIN_OUT_DIR = '.../out_SD/'\n",
    "SD_GET_EMBEDDINGS_SCRIPTS_PATH = '.../SD_get_embeddings_scripts/'\n",
    "\n",
    "for ds in atomic_num_dict.keys():\n",
    "    ### Type of graph layer/convolution.\n",
    "    for conv in ['GCN', 'GIN', 'PNA']:\n",
    "        ### Number of graph layers (depth of GNN).\n",
    "        for num_layers in [2, 3]:\n",
    "            ### Use VGAE (as in the paper) or a GNN alternative with no variational component. VGAE almost always works better for SD.\n",
    "            for vgae_type in ['VGAE', 'GNN']:\n",
    "                ### This is the Dense/MLP neural readout. The standard alternatives are 'global_add_pool', 'global_mean_pool', and 'global_max_pool'. The Set Transformer and GRU did not perform well on SD so they are not included at the moment.\n",
    "                for readout in ['linear']:\n",
    "                    ### Use edge features or not.\n",
    "                    for edges in ['--edge-dim', None]:\n",
    "\n",
    "                        ### GCN does not work with edges\n",
    "                        if conv == 'GCN' and edges is not None:\n",
    "                            continue\n",
    "                        \n",
    "                        vgae = '--use-vgae' if vgae_type == 'VGAE' else '--no-use-vgae'\n",
    "                        edges_path = 'EDGES' if edges is not None else 'NO-EDGES'\n",
    "\n",
    "                        gnn_interim_dim = 512 if conv != 'PNA' else 515\n",
    "\n",
    "                        out_dir = os.path.join(MAIN_OUT_DIR, f'{ds}/{num_layers}/{vgae_type}/{conv}/{readout}/{edges_path}')\n",
    "                        ### Out directory is the same as the checkpoint search directory\n",
    "                        ### Search for the latest (most recent) checkpoint in the directory defined previously for SD training\n",
    "                        ckpt_path = os.path.join(MAIN_OUT_DIR, f'{ds}/{num_layers}/{vgae_type}/{conv}/{readout}/{edges_path}')\n",
    "                        list_of_files = glob.glob(f'{ckpt_path}/*.ckpt')\n",
    "                        if len(list_of_files) > 0:\n",
    "                            latest_ckpt = max(list_of_files, key=os.path.getctime)\n",
    "\n",
    "                            ### Set input data path. This should be a single .csv file.\n",
    "                            data_path = SD_DATASET_FILE_PATH.format(ds=ds)\n",
    "\n",
    "                            ### Generate script with all the values \n",
    "                            script = script_template.format(ds=ds, data_path=data_path, vgae=vgae, conv=conv, num_layers=num_layers, readout=readout, \n",
    "                                out_dir=out_dir, max_atomic_number=atomic_num_dict[ds], gnn_interim_dim=gnn_interim_dim, sd_label=SD_LABEL, \n",
    "                                training_script_path=SD_GET_EMBEDDINGS_SCRIPT_PATH, id_column=ID_COLUMN)\n",
    "\n",
    "                            ### Add edges if value is set\n",
    "                            if edges_path == 'EDGES':\n",
    "                                script += ' --edge-dim 13'\n",
    "\n",
    "                            ### GPU or CPU\n",
    "                            if USE_CUDA:\n",
    "                                script += ' --use-cuda'\n",
    "                            else:\n",
    "                                script += ' --no-use-cuda'\n",
    "\n",
    "                            ### Add checkpoint path\n",
    "                            script += f' --ckpt-path {latest_ckpt}'\n",
    "                            \n",
    "                            ### Save training scripts in a dedicated directory.\n",
    "                            with open(os.path.join(SD_GET_EMBEDDINGS_SCRIPTS_PATH, f'{ds}_{num_layers}_{conv}_{vgae_type}_{readout}_{edges_path}.sh'), 'w') as f:\n",
    "                                f.write(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-geometric-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9 | packaged by conda-forge | (main, Dec 20 2021, 02:40:17) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "92c3512d332dfecc446af173ea486b2aef77501f9d1646c0b7132679fcde3a4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
