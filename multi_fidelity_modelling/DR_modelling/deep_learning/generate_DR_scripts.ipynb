{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from rdkit import Chem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This builds a list of directories which have the DR split data inside. DR split data means the 5 random splits in our case.\n",
    "dr_dirs = list(set(['/'.join(str(p).split('/')[:-2]) for p in Path('.../train_val_test_splits').rglob('*.csv') if 'ipynb' not in str(p)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dr_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_for_unsplit_dataset(unsplit_dataset_path: str, is_dr_separate: bool):\n",
    "    if is_dr_separate:\n",
    "        return pd.read_csv(unsplit_dataset_path)\n",
    "    else:\n",
    "        df = pd.read_csv(unsplit_dataset_path)\n",
    "        return df[~df['DR'].isna()].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_max_atomic_num(df):\n",
    "    smiles = df['neut-smiles'].values\n",
    "    mols = [Chem.MolFromSmiles(s) for s in smiles]\n",
    "    max_atomic_nums = []\n",
    "    max_num_atoms = []\n",
    "\n",
    "    for m in mols:\n",
    "        atom_nums = [a.GetAtomicNum() for a in m.GetAtoms()]\n",
    "        max_atomic_nums.append(np.max(atom_nums))\n",
    "        \n",
    "        num_atoms = len(m.GetAtoms())\n",
    "        max_num_atoms.append(num_atoms)\n",
    "    \n",
    "    df['Largest atomic number'] = np.max(max_atomic_nums)\n",
    "    df['# atoms'] = np.max(max_num_atoms)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DR_PYTHON_SCRIPT_PATH = '.../train_dr.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = 'python {training_script_path} --data-path {data_path} --out-dir {out_dir} --target-label {target_label} --node-latent-dim {node_dim} --graph-latent-dim {graph_dim} --smiles-column {smiles_column} --max-atomic-number {max_atomic_number} --readout {readout} --id-column {id_column} {use_vgae} --num-layers {num_layers} --conv {conv} {use_batch_norm} --gnn-intermediate-dim {gnn_interim_dim} --name {name} --task-type {task_type}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n"
     ]
    }
   ],
   "source": [
    "NODE_DIM = 50\n",
    "EMBEDDINGS_AUX_DIM = 64\n",
    "BATCH_NORM = '--use-batch-norm'\n",
    "\n",
    "MAIN_OUT_DIR = '.../out/'\n",
    "DR_TRAINING_SCRIPTS_PATH = '.../DR_training_scripts/'\n",
    "\n",
    "# Must be columns in the .csv files\n",
    "TARGET_LABEL = 'DR'\n",
    "SMILES_COL = 'neut-smiles'\n",
    "ID_COLUMN = 'CID'\n",
    "\n",
    "\n",
    "### CUDA disabled by default as these models are trained quickly. \n",
    "### PNA requires GPUs as it is very expensive, but GCN and GIN work well with CPUs.\n",
    "USE_CUDA = False\n",
    "\n",
    "for dr_dir in tqdm(dr_dirs):\n",
    "    ### Extracts the dataset name from the path, e.g. 'DR=AID1431-SD=AID873'.\n",
    "    dr_sd_name = dr_dir.split('/')[-1]\n",
    "\n",
    "    ### The entire DR file, without any splits. This is used to get extract some additional information.\n",
    "    dr_df = get_df_for_unsplit_dataset(f'.../AID{dr_sd_name}/SD.csv', is_dr_separate=False)\n",
    "    dr_dr = compute_max_atomic_num(dr_df)\n",
    "\n",
    "    ### Same arguments as in the SD notebook.\n",
    "    for conv in ['GCN', 'GIN', 'PNA']:\n",
    "        for vgae in ['--use-vgae', '--no-use-vgae']:\n",
    "            for num_layers in [2, 3, 4]:\n",
    "                for agg in ['global_add_pool', 'global_mean_pool', 'global_max_pool', 'linear']:\n",
    "                    for sd_label in ['Z-Score', 'Embeddings', None]:\n",
    "                        for itr in range(5):\n",
    "                            for edges in ['--edge-dim 13', None]:\n",
    "\n",
    "                                if conv == 'GCN' and edges:\n",
    "                                    continue\n",
    "\n",
    "                                if sd_label in ['Z-Score']:\n",
    "                                    aux_dim = 1\n",
    "                                    lbl_or_emb = 'lbl'\n",
    "                                elif sd_label == 'Embeddings':\n",
    "                                    lbl_or_emb = 'emb'\n",
    "                                    aux_dim = EMBEDDINGS_AUX_DIM\n",
    "                                else:\n",
    "                                    aux_dim = 0\n",
    "                                    lbl_or_emb = None\n",
    "                            \n",
    "                                GRAPH_DIM = 65 if conv == 'PNA' else 64\n",
    "                                GNN_INTERIM_DIM = 130 if conv == 'PNA' else 128\n",
    "\n",
    "                                ### Input data path. Expect a directory path (not path to file) where 3 files are available: train.csv, validate.csv, test.csv. \n",
    "                                ### This is hard-coded in train_dr.py.\n",
    "                                data_path = f'{dr_dir}/{itr}'\n",
    "\n",
    "                                vgae_type = 'VGAE' if vgae == '--use-vgae' else 'GNN'\n",
    "                                edges_type = 'EDGES' if edges else 'NO-EDGES'\n",
    "\n",
    "                                ### Output directory. The DR code only saves a single checkpoint (best according to the validation loss) and the test set predictions, true values, and metrics.\n",
    "                                ### This will be created if it does not exist.\n",
    "                                out_dir = os.path.join(MAIN_OUT_DIR, f'{dr_sd_name}/{conv}/{vgae_type}/{num_layers}/{agg}/{sd_label}/{itr}/{edges_type}')\n",
    "\n",
    "                                ### If ussing classification datasets.\n",
    "                                # task_type = 'classification'\n",
    "\n",
    "                                task_type = 'regression'\n",
    "                                \n",
    "                                max_atomic_num = dr_df['Largest atomic number'].values[0]\n",
    "                                max_mol_size = np.max(dr_df['# atoms'])\n",
    "\n",
    "                                script = template.format(data_path=data_path, out_dir=out_dir, target_label=TARGET_LABEL, node_dim=NODE_DIM, graph_dim=GRAPH_DIM, \n",
    "                                    smiles_column=SMILES_COL, max_atomic_number=max_atomic_num, readout=agg, id_column=ID_COLUMN, num_layers=num_layers, conv=conv, \n",
    "                                    gnn_interim_dim=GNN_INTERIM_DIM, name=dr_sd_name, lbl_or_emb=lbl_or_emb, task_type=task_type, use_vgae=vgae, use_batch_norm=BATCH_NORM, \n",
    "                                    training_script_path=TRAIN_DR_PYTHON_SCRIPT_PATH)\n",
    "\n",
    "                                if sd_label:\n",
    "                                    script += f' --lbl-or-emb {lbl_or_emb}  --auxiliary-data-column-name {sd_label}  --auxiliary-dim {aux_dim}'\n",
    "\n",
    "                                if edges:\n",
    "                                    script += f' {edges}'\n",
    "\n",
    "                                ### GPU or CPU\n",
    "                                if USE_CUDA:\n",
    "                                    script += ' --use-cuda'\n",
    "                                else:\n",
    "                                    script += ' --no-use-cuda'\n",
    "\n",
    "                                ### Save training scripts in a dedicated directory.\n",
    "                                with open(os.path.join(DR_TRAINING_SCRIPTS_PATH, f'{dr_sd_name}+{conv}+{vgae_type}+{num_layers}+{agg}+{sd_label}+{itr}+{edges_type}.txt'), 'w') as f:\n",
    "                                    f.write(script)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-geometric-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9 | packaged by conda-forge | (main, Dec 20 2021, 02:40:17) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "92c3512d332dfecc446af173ea486b2aef77501f9d1646c0b7132679fcde3a4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
